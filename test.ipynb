{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64370e4",
   "metadata": {},
   "source": [
    "# To develop and test the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02a592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance pandas numpy networkx nltk scikit-learn statsmodels transformers newspaper3k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b70aa257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nemesis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# News-Driven Cointegrated Stock Pairs Finder\n",
    "\n",
    "# =======================================\n",
    "# STEP 1: Install & Import Dependencies\n",
    "# =======================================\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "from newspaper import Article\n",
    "from newspaper import build\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aa04eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top connected pairs:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# STEP 2: Scrape and Process News Articles\n",
    "# =======================================\n",
    "def scrape_news_articles(source_url, max_articles=30):\n",
    "    paper = build(source_url, memoize_articles=False)\n",
    "    articles = []\n",
    "    for article in paper.articles[:max_articles]:\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            articles.append({\"title\": article.title, \"text\": article.text})\n",
    "        except:\n",
    "            continue\n",
    "    return articles\n",
    "\n",
    "# Example: Reuters\n",
    "news_articles = scrape_news_articles('https://www.reuters.com')\n",
    "\n",
    "# =======================================\n",
    "# STEP 3: Extract Tickers / Company Mentions\n",
    "# =======================================\n",
    "# Simplified static list for demo; replace with Named Entity Linking later\n",
    "company_tickers = {\n",
    "    'Apple': 'AAPL', 'Microsoft': 'MSFT', 'Google': 'GOOGL',\n",
    "    'Amazon': 'AMZN', 'Meta': 'META', 'Tesla': 'TSLA',\n",
    "    'Nvidia': 'NVDA', 'AMD': 'AMD', 'Intel': 'INTC', 'Netflix': 'NFLX'\n",
    "}\n",
    "\n",
    "def extract_mentions(text):\n",
    "    mentioned = []\n",
    "    for name in company_tickers:\n",
    "        if re.search(rf'\\b{name}\\b', text, re.IGNORECASE):\n",
    "            mentioned.append(company_tickers[name])\n",
    "    return mentioned\n",
    "\n",
    "co_mentions = []\n",
    "for article in news_articles:\n",
    "    mentions = extract_mentions(article['text'])\n",
    "    if len(mentions) > 1:\n",
    "        co_mentions.append(mentions)\n",
    "        \n",
    "\n",
    "\n",
    "# =======================================\n",
    "# STEP 4: Build Co-occurrence Graph\n",
    "# =======================================\n",
    "G = nx.Graph()\n",
    "for pair_list in co_mentions:\n",
    "    for i in range(len(pair_list)):\n",
    "        for j in range(i+1, len(pair_list)):\n",
    "            a, b = pair_list[i], pair_list[j]\n",
    "            if G.has_edge(a, b):\n",
    "                G[a][b]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(a, b, weight=1)\n",
    "\n",
    "# Visualize top co-occurrences\n",
    "print(\"Top connected pairs:\")\n",
    "print(sorted(G.edges(data=True), key=lambda x: -x[2]['weight'])[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33da111e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'n_clusters' parameter of SpectralClustering must be an int in the range [1, inf). Got 0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m n_clusters = \u001b[38;5;28mmin\u001b[39m(\u001b[32m3\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tickers))\n\u001b[32m      9\u001b[39m clustering = SpectralClustering(n_clusters=n_clusters, affinity=\u001b[33m'\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m labels = \u001b[43mclustering\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m clusters = {}\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Interviews/Quant_project/.venv/lib/python3.13/site-packages/sklearn/cluster/_spectral.py:793\u001b[39m, in \u001b[36mSpectralClustering.fit_predict\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    772\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform spectral clustering on `X` and return cluster labels.\u001b[39;00m\n\u001b[32m    773\u001b[39m \n\u001b[32m    774\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    791\u001b[39m \u001b[33;03m        Cluster labels.\u001b[39;00m\n\u001b[32m    792\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Interviews/Quant_project/.venv/lib/python3.13/site-packages/sklearn/base.py:719\u001b[39m, in \u001b[36mClusterMixin.fit_predict\u001b[39m\u001b[34m(self, X, y, **kwargs)\u001b[39m\n\u001b[32m    696\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[33;03mPerform clustering on `X` and returns cluster labels.\u001b[39;00m\n\u001b[32m    698\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    715\u001b[39m \u001b[33;03m    Cluster labels.\u001b[39;00m\n\u001b[32m    716\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    717\u001b[39m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[32m    718\u001b[39m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Interviews/Quant_project/.venv/lib/python3.13/site-packages/sklearn/base.py:1382\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1377\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1378\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1379\u001b[39m )\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Interviews/Quant_project/.venv/lib/python3.13/site-packages/sklearn/base.py:436\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    429\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    430\u001b[39m \n\u001b[32m    431\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Interviews/Quant_project/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'n_clusters' parameter of SpectralClustering must be an int in the range [1, inf). Got 0 instead."
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# STEP 5: Cluster Related Stocks\n",
    "# =======================================\n",
    "# Convert to adjacency matrix\n",
    "adj_matrix = nx.to_numpy_array(G)\n",
    "tickers = list(G.nodes)\n",
    "\n",
    "n_clusters = min(3, len(tickers))\n",
    "clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed')\n",
    "labels = clustering.fit_predict(adj_matrix)\n",
    "\n",
    "clusters = {}\n",
    "for i, label in enumerate(labels):\n",
    "    clusters.setdefault(label, []).append(tickers[i])\n",
    "\n",
    "print(\"\\nIdentified Clusters:\")\n",
    "print(clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0be826f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pvalue\n\u001b[32m     12\u001b[39m results = []\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclusters\u001b[49m.values():\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cluster)):\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i+\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cluster)):\n",
      "\u001b[31mNameError\u001b[39m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# STEP 6: Test Cointegration in Clusters\n",
    "# =======================================\n",
    "def test_cointegration(ticker1, ticker2):\n",
    "    data = yf.download([ticker1, ticker2], start=\"2022-01-01\")['Adj Close'].dropna()\n",
    "    if len(data.columns) < 2:\n",
    "        return None\n",
    "    series1, series2 = data.iloc[:, 0], data.iloc[:, 1]\n",
    "    score, pvalue, _ = coint(series1, series2)\n",
    "    return pvalue\n",
    "\n",
    "results = []\n",
    "for cluster in clusters.values():\n",
    "    for i in range(len(cluster)):\n",
    "        for j in range(i+1, len(cluster)):\n",
    "            t1, t2 = cluster[i], cluster[j]\n",
    "            pval = test_cointegration(t1, t2)\n",
    "            if pval is not None:\n",
    "                results.append((t1, t2, pval))\n",
    "\n",
    "cointegrated_pairs = [(a, b, p) for (a, b, p) in results if p < 0.05]\n",
    "\n",
    "print(\"\\nLikely Cointegrated Pairs:\")\n",
    "print(cointegrated_pairs)\n",
    "\n",
    "# =======================================\n",
    "# (Optional) STEP 7: Sentiment Analysis on Articles\n",
    "# =======================================\n",
    "# For simplicity, we skip this here — in the repo we will use FinBERT\n",
    "\n",
    "# END OF NOTEBOOK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d2ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5f45c3e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da9d1222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import logging\n",
    "\n",
    "# import my modules\n",
    "import etl.extract as ext\n",
    "import etl.load as ld\n",
    "import etl.transform as tfm\n",
    "from utils.logger import setup_logger\n",
    "\n",
    "\n",
    "# Extract\n",
    "df_raw = ext.extract_tweets(\"data/stock_tweets/tweets_small.csv\")\n",
    "\n",
    "# Transform\n",
    "df_valid, df_clean = tfm.transform(df_raw)\n",
    "# tfm.transform(df_raw)\n",
    "\n",
    "# Load\n",
    "ld.load_to_sqlite(df_clean, db_path=\"data/tweets.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ca9b4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "post_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "retweet_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "like_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0c44b735-e924-4598-b394-442faea1951e",
       "rows": [
        [
         "0",
         "550441509175443456",
         "VisualStockRSRC",
         "2015-01-01T00:00:57",
         "lx21 made $10,008  on $AAPL -Check it out! http://profit.ly/1MnD8s?aff=202 Learn #howtotrade http://bit.ly/1c1NljX $EXE $WATT $IMRS $CACH $GMO",
         "0",
         "0",
         "1",
         "lx21 made $10,008  on $aapl -check it out!  learn   $exe $watt $imrs $cach $gmo",
         "['lx21', 'made', '10,008', 'aapl', '-check', 'learn', 'exe', 'watt', 'imrs', 'cach', 'gmo']"
        ],
        [
         "1",
         "550441672312512512",
         "KeralaGuy77",
         "2015-01-01T00:01:36",
         "Insanity of today weirdo massive selling. $aapl bid up 45 cents after hours after non stop selling in trading hours",
         "0",
         "0",
         "0",
         "insanity of today weirdo massive selling. $aapl bid up 45 cents after hours after non stop selling in trading hours",
         "['insanity', 'today', 'weirdo', 'massive', 'selling', 'aapl', 'bid', 'cents', 'hours', 'non', 'stop', 'selling', 'trading', 'hours']"
        ],
        [
         "2",
         "550441732014223360",
         "DozenStocks",
         "2015-01-01T00:01:50",
         "S&P100 #Stocks Performance $HD $LOW $SBUX $TGT $DVN $IBM $AMZN $F $APA $GM $MS $HAL $DIS $MCD $BMY $XOM  more@ http://12Stocks.com/sp100",
         "0",
         "0",
         "0",
         "s&p100  performance $hd $low $sbux $tgt $dvn $ibm $amzn $f $apa $gm $ms $hal $dis $mcd $bmy $xom  more@",
         "['p100', 'performance', 'low', 'sbux', 'tgt', 'dvn', 'ibm', 'amzn', 'apa', 'hal', 'dis', 'mcd', 'bmy', 'xom']"
        ],
        [
         "3",
         "550442977802207232",
         "ShowDreamCar",
         "2015-01-01T00:06:47",
         "$GM $TSLA: Volkswagen Pushes 2014 Record Recall Tally Higher https://pic.twitter.com/WIIc1lW7hW @ProTradersNews http://growword.com/2015/01/01/0246.html… @theferrarifan",
         "0",
         "0",
         "1",
         "$gm $tsla: volkswagen pushes 2014 record recall tally higher",
         "['tsla', 'volkswagen', 'pushes', '2014', 'record', 'recall', 'tally', 'higher']"
        ],
        [
         "4",
         "550443807834402816",
         "i_Know_First",
         "2015-01-01T00:10:05",
         "Swing Trading: Up To 8.91% Return In 14 Days http://ow.ly/GDks0 #swingtrading #forecast #techstock $MWW $AAPL $TSLA",
         "0",
         "0",
         "1",
         "swing trading: up to 8.91% return in 14 days     $mww $aapl $tsla",
         "['swing', 'trading', '8.91', 'return', 'days', 'mww', 'aapl', 'tsla']"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>post_date</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_num</th>\n",
       "      <th>retweet_num</th>\n",
       "      <th>like_num</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550441509175443456</td>\n",
       "      <td>VisualStockRSRC</td>\n",
       "      <td>2015-01-01T00:00:57</td>\n",
       "      <td>lx21 made $10,008  on $AAPL -Check it out! htt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>lx21 made $10,008  on $aapl -check it out!  le...</td>\n",
       "      <td>[lx21, made, 10,008, aapl, -check, learn, exe,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550441672312512512</td>\n",
       "      <td>KeralaGuy77</td>\n",
       "      <td>2015-01-01T00:01:36</td>\n",
       "      <td>Insanity of today weirdo massive selling. $aap...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>insanity of today weirdo massive selling. $aap...</td>\n",
       "      <td>[insanity, today, weirdo, massive, selling, aa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550441732014223360</td>\n",
       "      <td>DozenStocks</td>\n",
       "      <td>2015-01-01T00:01:50</td>\n",
       "      <td>S&amp;P100 #Stocks Performance $HD $LOW $SBUX $TGT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>s&amp;p100  performance $hd $low $sbux $tgt $dvn $...</td>\n",
       "      <td>[p100, performance, low, sbux, tgt, dvn, ibm, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550442977802207232</td>\n",
       "      <td>ShowDreamCar</td>\n",
       "      <td>2015-01-01T00:06:47</td>\n",
       "      <td>$GM $TSLA: Volkswagen Pushes 2014 Record Recal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>$gm $tsla: volkswagen pushes 2014 record recal...</td>\n",
       "      <td>[tsla, volkswagen, pushes, 2014, record, recal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>550443807834402816</td>\n",
       "      <td>i_Know_First</td>\n",
       "      <td>2015-01-01T00:10:05</td>\n",
       "      <td>Swing Trading: Up To 8.91% Return In 14 Days h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>swing trading: up to 8.91% return in 14 days  ...</td>\n",
       "      <td>[swing, trading, 8.91, return, days, mww, aapl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           author            post_date  \\\n",
       "0  550441509175443456  VisualStockRSRC  2015-01-01T00:00:57   \n",
       "1  550441672312512512      KeralaGuy77  2015-01-01T00:01:36   \n",
       "2  550441732014223360      DozenStocks  2015-01-01T00:01:50   \n",
       "3  550442977802207232     ShowDreamCar  2015-01-01T00:06:47   \n",
       "4  550443807834402816     i_Know_First  2015-01-01T00:10:05   \n",
       "\n",
       "                                                body  comment_num  \\\n",
       "0  lx21 made $10,008  on $AAPL -Check it out! htt...            0   \n",
       "1  Insanity of today weirdo massive selling. $aap...            0   \n",
       "2  S&P100 #Stocks Performance $HD $LOW $SBUX $TGT...            0   \n",
       "3  $GM $TSLA: Volkswagen Pushes 2014 Record Recal...            0   \n",
       "4  Swing Trading: Up To 8.91% Return In 14 Days h...            0   \n",
       "\n",
       "   retweet_num  like_num                                         clean_text  \\\n",
       "0            0         1  lx21 made $10,008  on $aapl -check it out!  le...   \n",
       "1            0         0  insanity of today weirdo massive selling. $aap...   \n",
       "2            0         0  s&p100  performance $hd $low $sbux $tgt $dvn $...   \n",
       "3            0         1  $gm $tsla: volkswagen pushes 2014 record recal...   \n",
       "4            0         1  swing trading: up to 8.91% return in 14 days  ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [lx21, made, 10,008, aapl, -check, learn, exe,...  \n",
       "1  [insanity, today, weirdo, massive, selling, aa...  \n",
       "2  [p100, performance, low, sbux, tgt, dvn, ibm, ...  \n",
       "3  [tsla, volkswagen, pushes, 2014, record, recal...  \n",
       "4  [swing, trading, 8.91, return, days, mww, aapl...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "def safe_json_load(x):\n",
    "    try:\n",
    "        if isinstance(x, str) and x.strip().startswith('['):\n",
    "            return json.loads(x)\n",
    "        return x\n",
    "    except json.JSONDecodeError:\n",
    "        return [] \n",
    "    \n",
    "# Read sqlite query results into a pandas DataFrame\n",
    "conn = sqlite3.connect(\"data/tweets.db\")\n",
    "df = pd.read_sql_query(\"SELECT * from processed_tweets\", conn)\n",
    "\n",
    "# Apply to column\n",
    "df['tokens'] = df['tokens'].apply(safe_json_load)\n",
    "\n",
    "conn.close()\n",
    "# Verify that result of SQL query is stored in the dataframe\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8397b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
