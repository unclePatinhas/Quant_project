{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9324eb7",
   "metadata": {},
   "source": [
    "# Cointegrated stock pairs finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance pandas numpy networkx nltk scikit-learn statsmodels transformers newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# News-Driven Cointegrated Stock Pairs Finder\n",
    "\n",
    "# =======================================\n",
    "# STEP 1: Install & Import Dependencies\n",
    "# =======================================\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "from newspaper import Article\n",
    "from newspaper import build\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# STEP 2: Scrape and Process News Articles\n",
    "# =======================================\n",
    "def scrape_news_articles(source_url, max_articles=30):\n",
    "    paper = build(source_url, memoize_articles=False)\n",
    "    articles = []\n",
    "    for article in paper.articles[:max_articles]:\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            articles.append({\"title\": article.title, \"text\": article.text})\n",
    "        except:\n",
    "            continue\n",
    "    return articles\n",
    "\n",
    "# Example: Reuters\n",
    "news_articles = scrape_news_articles('https://www.reuters.com')\n",
    "\n",
    "# =======================================\n",
    "# STEP 3: Extract Tickers / Company Mentions\n",
    "# =======================================\n",
    "# Simplified static list for demo; replace with Named Entity Linking later\n",
    "company_tickers = {\n",
    "    'Apple': 'AAPL', 'Microsoft': 'MSFT', 'Google': 'GOOGL',\n",
    "    'Amazon': 'AMZN', 'Meta': 'META', 'Tesla': 'TSLA',\n",
    "    'Nvidia': 'NVDA', 'AMD': 'AMD', 'Intel': 'INTC', 'Netflix': 'NFLX'\n",
    "}\n",
    "\n",
    "def extract_mentions(text):\n",
    "    mentioned = []\n",
    "    for name in company_tickers:\n",
    "        if re.search(rf'\\b{name}\\b', text, re.IGNORECASE):\n",
    "            mentioned.append(company_tickers[name])\n",
    "    return mentioned\n",
    "\n",
    "co_mentions = []\n",
    "for article in news_articles:\n",
    "    mentions = extract_mentions(article['text'])\n",
    "    if len(mentions) > 1:\n",
    "        co_mentions.append(mentions)\n",
    "        \n",
    "\n",
    "\n",
    "# =======================================\n",
    "# STEP 4: Build Co-occurrence Graph\n",
    "# =======================================\n",
    "G = nx.Graph()\n",
    "for pair_list in co_mentions:\n",
    "    for i in range(len(pair_list)):\n",
    "        for j in range(i+1, len(pair_list)):\n",
    "            a, b = pair_list[i], pair_list[j]\n",
    "            if G.has_edge(a, b):\n",
    "                G[a][b]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(a, b, weight=1)\n",
    "\n",
    "# Visualize top co-occurrences\n",
    "print(\"Top connected pairs:\")\n",
    "print(sorted(G.edges(data=True), key=lambda x: -x[2]['weight'])[:5])\n",
    "\n",
    "# =======================================\n",
    "# STEP 5: Cluster Related Stocks\n",
    "# =======================================\n",
    "# Convert to adjacency matrix\n",
    "adj_matrix = nx.to_numpy_array(G)\n",
    "tickers = list(G.nodes)\n",
    "\n",
    "n_clusters = min(3, len(tickers))\n",
    "clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed')\n",
    "labels = clustering.fit_predict(adj_matrix)\n",
    "\n",
    "clusters = {}\n",
    "for i, label in enumerate(labels):\n",
    "    clusters.setdefault(label, []).append(tickers[i])\n",
    "\n",
    "print(\"\\nIdentified Clusters:\")\n",
    "print(clusters)\n",
    "\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# STEP 6: Test Cointegration in Clusters\n",
    "# =======================================\n",
    "def test_cointegration(ticker1, ticker2):\n",
    "    data = yf.download([ticker1, ticker2], start=\"2022-01-01\")['Adj Close'].dropna()\n",
    "    if len(data.columns) < 2:\n",
    "        return None\n",
    "    series1, series2 = data.iloc[:, 0], data.iloc[:, 1]\n",
    "    score, pvalue, _ = coint(series1, series2)\n",
    "    return pvalue\n",
    "\n",
    "results = []\n",
    "for cluster in clusters.values():\n",
    "    for i in range(len(cluster)):\n",
    "        for j in range(i+1, len(cluster)):\n",
    "            t1, t2 = cluster[i], cluster[j]\n",
    "            pval = test_cointegration(t1, t2)\n",
    "            if pval is not None:\n",
    "                results.append((t1, t2, pval))\n",
    "\n",
    "cointegrated_pairs = [(a, b, p) for (a, b, p) in results if p < 0.05]\n",
    "\n",
    "print(\"\\nLikely Cointegrated Pairs:\")\n",
    "print(cointegrated_pairs)\n",
    "\n",
    "# =======================================\n",
    "# (Optional) STEP 7: Sentiment Analysis on Articles\n",
    "# =======================================\n",
    "# For simplicity, we skip this here â€” in the repo we will use FinBERT\n",
    "\n",
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
